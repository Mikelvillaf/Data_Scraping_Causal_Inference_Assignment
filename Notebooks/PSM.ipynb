{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "423da7bc",
   "metadata": {},
   "source": [
    "# Causal Inference Analysis: Effect of High Discount Percentage on Product Star Rating\n",
    "\n",
    "**Objective**: To estimate the causal effect of a product having a \"High Discount Percentage\" on its average customer star `rating` on MercadoLibre.\n",
    "\n",
    "**Methodology**: We will use Propensity Score Matching (PSM) to create comparable groups of products (those with a high discount vs. those with a low/no discount) within a specific product category. This will help control for confounding variables such as current price, free delivery status, seller verification, total review count, and the presence of additional discount coupons.\n",
    "\n",
    "**Outcome Variable (Y)**: `rating` (Product star rating)\n",
    "**Treatment Variable (X)**: `is_high_discount` (Binary: 1 if discount is high, 0 if low/none)\n",
    "**Covariates (Z)**: `current_price`, `has_free_delivery`, `is_verified_seller`, `total_review_count`, `has_additional_discount_coupon`\n",
    "\n",
    "**Data Source**: Scraped data from MercadoLibre products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7339419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# np.float = float # Keep this if needed for older packages, otherwise, ensure your libraries are up to date.\n",
    "                # Modern numpy versions use np.float_ or just float.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import ttest_ind, chi2_contingency # Added chi2_contingency for binary covariates\n",
    "from statsmodels.stats.proportion import proportions_ztest # Alternative for binary\n",
    "\n",
    "# For consistent styling of outputs (optional)\n",
    "pd.options.display.float_format = '{:.3f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1e23d7",
   "metadata": {},
   "source": [
    "## Load, Inspect, and Prepare Data\n",
    "\n",
    "1.  **Load the dataset**: Update with your actual filename.\n",
    "2.  **Filter for a specific Product Category**: This is crucial for meaningful analysis.\n",
    "3.  **Create Discount Percentage Variable**: If not already scraped directly.\n",
    "4.  **Create Binary Treatment Variable (`is_high_discount`)**.\n",
    "5.  **Rename columns for clarity** if needed (e.g., `new_price` to `current_price`, `review_count` to `total_review_count`).\n",
    "6.  **Handle Missing Values**: Address NaNs in treatment, outcome, and covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8101b301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: data/trialrun/mercadolibre_solar_cream.csv\n",
      "Successfully loaded dataset. Shape: (633, 10)\n",
      "\n",
      "Calculated 'discount_percentage'.\n",
      "Created 'has_free_delivery' column.\n",
      "Created 'is_verified_seller' column.\n",
      "Created 'has_additional_discount_coupon' column.\n",
      "Renamed columns: 'new_price' to 'current_price', 'review_count' to 'total_review_count'.\n",
      "\n",
      "Discount threshold for 'is_high_discount': 15.04%\n",
      "Distribution of 'is_high_discount':\n",
      "is_high_discount\n",
      "0    347\n",
      "1    286\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Shape of analysis DataFrame before dropping NaNs: (633, 7)\n",
      "Shape of analysis DataFrame after dropping NaNs: (633, 7)\n",
      "\n",
      "Final prepared DataFrame for PSM (df_analysis) head:\n",
      "   rating  is_high_discount  current_price  has_free_delivery  \\\n",
      "0   4.400                 1         52.000                  0   \n",
      "1   5.000                 1        489.000                  1   \n",
      "2   5.000                 0        522.000                  1   \n",
      "3   5.000                 1        182.000                  0   \n",
      "4   5.000                 0        483.000                  1   \n",
      "\n",
      "   is_verified_seller  total_review_count  has_additional_discount_coupon  \n",
      "0                   0                  40                               0  \n",
      "1                   0                   1                               0  \n",
      "2                   0                   2                               0  \n",
      "3                   0                   1                               0  \n",
      "4                   0                   4                               0  \n",
      "\n",
      "Info for df_analysis:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 633 entries, 0 to 632\n",
      "Data columns (total 7 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   rating                          633 non-null    float64\n",
      " 1   is_high_discount                633 non-null    int64  \n",
      " 2   current_price                   633 non-null    float64\n",
      " 3   has_free_delivery               633 non-null    int64  \n",
      " 4   is_verified_seller              633 non-null    int64  \n",
      " 5   total_review_count              633 non-null    int64  \n",
      " 6   has_additional_discount_coupon  633 non-null    int64  \n",
      "dtypes: float64(2), int64(5)\n",
      "memory usage: 34.7 KB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the input CSV path\n",
    "DATA_CSV_PATH = 'data/trialrun/mercadolibre_solar_cream.csv'\n",
    "\n",
    "print(f\"Loading dataset from: {DATA_CSV_PATH}\")\n",
    "try:\n",
    "    df = pd.read_csv(DATA_CSV_PATH)\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Loaded DataFrame is empty. Please check the CSV file.\")\n",
    "    print(f\"Successfully loaded dataset. Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Error: Dataset file '{DATA_CSV_PATH}' not found. Please ensure the file is in the correct location and you have uploaded it.\")\n",
    "\n",
    "# Ensure price columns are numeric\n",
    "df['old_price'] = pd.to_numeric(df['old_price'], errors='coerce')\n",
    "df['new_price'] = pd.to_numeric(df['new_price'], errors='coerce')\n",
    "\n",
    "# 1. Calculate 'discount_percentage'\n",
    "df['discount_percentage'] = 0.0\n",
    "# Calculate discount only where old_price is valid, positive, and greater than new_price\n",
    "valid_discount_mask = (df['old_price'].notna()) & \\\n",
    "                    (df['new_price'].notna()) & \\\n",
    "                    (df['old_price'] > 0) & \\\n",
    "                    (df['old_price'] > df['new_price'])\n",
    "df.loc[valid_discount_mask, 'discount_percentage'] = \\\n",
    "    ((df['old_price'] - df['new_price']) / df['old_price']) * 100\n",
    "df['discount_percentage'].fillna(0, inplace=True) # Ensure no NaNs if prices were not coercible\n",
    "print(\"\\nCalculated 'discount_percentage'.\")\n",
    "\n",
    "# 2. Create 'has_free_delivery' (binary)\n",
    "# Based on the 'free_shipping' column\n",
    "df['has_free_delivery'] = np.where(\n",
    "    df['free_shipping'].astype(str).str.contains(\"EnvÃ­o gratis\", case=False, na=False), 1, 0\n",
    ")\n",
    "print(\"Created 'has_free_delivery' column.\")\n",
    "\n",
    "# 3. Create 'is_verified_seller' (binary)\n",
    "# Based on the 'official_seller' column: 0 if \"Not Found\", 1 otherwise\n",
    "df['is_verified_seller'] = np.where(\n",
    "    df['official_seller'].astype(str).str.strip().fillna(\"Not Found\").str.upper() == \"NOT FOUND\", 0, 1\n",
    ")\n",
    "print(\"Created 'is_verified_seller' column.\")\n",
    "\n",
    "# 4. Create 'has_additional_discount_coupon' (binary)\n",
    "# Based on the 'discount' column in the CSV (not the percentage calculated above)\n",
    "# Assumes any non-zero value in 'discount' column indicates an additional distinct discount/coupon\n",
    "if 'discount' in df.columns:\n",
    "    df['has_additional_discount_coupon'] = np.where(pd.to_numeric(df['discount'], errors='coerce').fillna(0) > 0, 1, 0)\n",
    "    print(\"Created 'has_additional_discount_coupon' column.\")\n",
    "else:\n",
    "    print(\"Warning: Original 'discount' column not found for 'has_additional_discount_coupon'. Column set to 0.\")\n",
    "    df['has_additional_discount_coupon'] = 0\n",
    "\n",
    "# 5. Rename columns for PSM analysis\n",
    "df.rename(columns={\n",
    "    'new_price': 'current_price',\n",
    "    'review_count': 'total_review_count'\n",
    "}, inplace=True)\n",
    "print(\"Renamed columns: 'new_price' to 'current_price', 'review_count' to 'total_review_count'.\")\n",
    "\n",
    "# 6. Create Binary Treatment Variable (`is_high_discount`)\n",
    "# Define \"high discount\" based on the median of products *that actually have a discount*\n",
    "positive_discounts = df[df['discount_percentage'] > 0]['discount_percentage']\n",
    "if not positive_discounts.empty:\n",
    "    DISCOUNT_THRESHOLD = positive_discounts.median()\n",
    "else:\n",
    "    DISCOUNT_THRESHOLD = 0 # Fallback if no products have any discount\n",
    "    print(f\"Warning: No products with positive 'discount_percentage' values found. Threshold for 'is_high_discount' set to {DISCOUNT_THRESHOLD:.2f}%.\")\n",
    "\n",
    "df['is_high_discount'] = (df['discount_percentage'] > DISCOUNT_THRESHOLD).astype(int)\n",
    "print(f\"\\nDiscount threshold for 'is_high_discount': {DISCOUNT_THRESHOLD:.2f}%\")\n",
    "print(f\"Distribution of 'is_high_discount':\\n{df['is_high_discount'].value_counts(dropna=False)}\")\n",
    "\n",
    "# 7. Define outcome, treatment, and covariate lists\n",
    "outcome_variable = 'rating'\n",
    "treatment_variable = 'is_high_discount'\n",
    "covariates_list = [\n",
    "    'current_price',\n",
    "    'has_free_delivery',\n",
    "    'is_verified_seller',\n",
    "    'total_review_count',\n",
    "    'has_additional_discount_coupon'\n",
    "]\n",
    "\n",
    "# Ensure 'rating' column is numeric and other key columns are correctly typed\n",
    "if outcome_variable not in df.columns:\n",
    "    raise ValueError(f\"Outcome variable '{outcome_variable}' not found in DataFrame.\")\n",
    "df[outcome_variable] = pd.to_numeric(df[outcome_variable], errors='coerce')\n",
    "\n",
    "for col in ['current_price', 'total_review_count']:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    else:\n",
    "        # This should not happen if renaming was successful and columns existed\n",
    "        raise ValueError(f\"Essential covariate '{col}' is missing after renaming.\")\n",
    "\n",
    "# 8. Prepare final DataFrame for analysis (selecting analysis columns and handling NaNs)\n",
    "all_analysis_columns = [outcome_variable, treatment_variable] + covariates_list\n",
    "df_analysis = df[all_analysis_columns].copy()\n",
    "\n",
    "print(f\"\\nShape of analysis DataFrame before dropping NaNs: {df_analysis.shape}\")\n",
    "df_analysis.dropna(inplace=True) # Drop rows if any of the essential columns have NaN\n",
    "print(f\"Shape of analysis DataFrame after dropping NaNs: {df_analysis.shape}\")\n",
    "\n",
    "# 9. Final checks for PSM readiness\n",
    "if df_analysis.empty:\n",
    "    print(\"Warning: DataFrame for analysis is empty after processing and NaN removal. PSM cannot proceed.\")\n",
    "else:\n",
    "    if df_analysis.shape[0] < 30:\n",
    "        print(f\"Warning: Sample size for analysis is very small ({df_analysis.shape[0]}). PSM results may be unreliable.\")\n",
    "    if df_analysis[treatment_variable].nunique() < 2:\n",
    "        raise ValueError(\n",
    "            f\"Treatment variable '{treatment_variable}' does not have at least two distinct values (0 and 1) \"\n",
    "            f\"in the cleaned analysis data. Current value counts:\\n{df_analysis[treatment_variable].value_counts(dropna=False)}\\n\"\n",
    "            f\"PSM cannot proceed. This might be due to all products having low/no discount based on the threshold.\"\n",
    "        )\n",
    "\n",
    "print(f\"\\nFinal prepared DataFrame for PSM (df_analysis) head:\")\n",
    "if not df_analysis.empty:\n",
    "    print(df_analysis.head())\n",
    "    print(\"\\nInfo for df_analysis:\")\n",
    "    df_analysis.info()\n",
    "else:\n",
    "    print(\"df_analysis is empty.\")\n",
    "\n",
    "# You can now use df_analysis for the subsequent PSM steps.\n",
    "# If you need to save this specific prepped DataFrame:\n",
    "# df_analysis.to_csv('prepped_data_for_psm.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3292aacb",
   "metadata": {},
   "source": [
    "## Define Key Variables for the Analysis\n",
    "\n",
    "* **Outcome (Y)**: `rating` (Product's average star rating, continuous)\n",
    "* **Treatment (X)**: `is_high_discount` (Binary: 1 if the product has a discount percentage above the category threshold, 0 otherwise)\n",
    "* **Covariates (Z)**:\n",
    "    * `current_price` (Current selling price of the product)\n",
    "    * `has_free_delivery` (Binary: 1 if free delivery is offered, 0 otherwise)\n",
    "    * `is_verified_seller` (Binary: 1 if the seller is verified, 0 otherwise)\n",
    "    * `total_review_count` (Total number of reviews for the product)\n",
    "    * `has_additional_discount_coupon` (Binary: 1 if an additional coupon is offered, 0 otherwise)\n",
    "\n",
    "    ## Step 1: Check Covariate Balance Before Matching\n",
    "\n",
    "We will compare the means of continuous covariates and proportions of binary covariates between the 'High Discount' group and the 'Low/No Discount' group *before* applying propensity score matching. This helps us understand the initial differences between the groups and the necessity of matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2892d439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-match - Treated group size: 286\n",
      "Pre-match - Control group size: 347\n",
      "\n",
      "--- Continuous Covariate Balance Before Matching ---\n",
      "            Covariate Mean Treated Mean Control Std Diff (SMD) T-Statistic  \\\n",
      "0       current_price       358.51       377.95         -0.081       -1.06   \n",
      "1  total_review_count        39.42        13.00          0.168        1.92   \n",
      "\n",
      "  P-Value  \n",
      "0   0.288  \n",
      "1   0.056  \n",
      "\n",
      "--- Binary Covariate Balance Before Matching ---\n",
      "                        Covariate Prop Treated Prop Control Std Diff (SMD)  \\\n",
      "0               has_free_delivery         0.61         0.75         -0.305   \n",
      "1              is_verified_seller         0.20         0.19          0.023   \n",
      "2  has_additional_discount_coupon         0.00         0.00            nan   \n",
      "\n",
      "  Z-Statistic P-Value  \n",
      "0       -3.80   0.000  \n",
      "1        0.29   0.773  \n",
      "2         nan     nan  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5p/6h8y8yzn7rxdgn4rkplx53rh0000gn/T/ipykernel_3270/2940793195.py:71: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  smd_binary = (prop_treated - prop_control) / np.sqrt((prop_treated * (1 - prop_treated) + prop_control * (1 - prop_control)) / 2)\n",
      "/opt/anaconda3/lib/python3.11/site-packages/statsmodels/stats/weightstats.py:792: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  zstat = value / std\n"
     ]
    }
   ],
   "source": [
    "# Separate treated (is_high_discount == 1) and control (is_high_discount == 0) groups\n",
    "treated_group_pre_match = df[df[treatment_variable] == 1]\n",
    "control_group_pre_match = df[df[treatment_variable] == 0]\n",
    "\n",
    "if treated_group_pre_match.empty or control_group_pre_match.empty:\n",
    "    raise ValueError(f\"Not enough data in treatment or control group for category '{CATEGORY_TO_ANALYZE}' to proceed with balance checks.\")\n",
    "\n",
    "print(f\"Pre-match - Treated group size: {len(treated_group_pre_match)}\")\n",
    "print(f\"Pre-match - Control group size: {len(control_group_pre_match)}\")\n",
    "\n",
    "\n",
    "# --- Balance for Continuous Covariates ---\n",
    "continuous_covariates = ['current_price', 'total_review_count']\n",
    "print(\"\\n--- Continuous Covariate Balance Before Matching ---\")\n",
    "balance_results_continuous_pre = []\n",
    "for var in continuous_covariates:\n",
    "    if var in df.columns:\n",
    "        # Ensure there's data to compare\n",
    "        if not treated_group_pre_match[var].dropna().empty and not control_group_pre_match[var].dropna().empty:\n",
    "            mean_treated = treated_group_pre_match[var].mean()\n",
    "            mean_control = control_group_pre_match[var].mean()\n",
    "            std_treated = treated_group_pre_match[var].std()\n",
    "            std_control = control_group_pre_match[var].std()\n",
    "            \n",
    "            # Standardized Mean Difference (SMD)\n",
    "            # Pooled standard deviation for SMD calculation\n",
    "            s_pooled = np.sqrt(((len(treated_group_pre_match)-1)*(std_treated**2) + \\\n",
    "                                (len(control_group_pre_match)-1)*(std_control**2)) / \\\n",
    "                            (len(treated_group_pre_match) + len(control_group_pre_match) - 2))\n",
    "            smd = (mean_treated - mean_control) / s_pooled if s_pooled > 0 else 0\n",
    "            \n",
    "            t_stat, p_val = ttest_ind(treated_group_pre_match[var].dropna(), control_group_pre_match[var].dropna(), equal_var=False) # Welch's t-test\n",
    "            balance_results_continuous_pre.append({\n",
    "                'Covariate': var,\n",
    "                'Mean Treated': f\"{mean_treated:.2f}\",\n",
    "                'Mean Control': f\"{mean_control:.2f}\",\n",
    "                'Std Diff (SMD)': f\"{smd:.3f}\", # SMD is often preferred over p-values for balance\n",
    "                'T-Statistic': f\"{t_stat:.2f}\",\n",
    "                'P-Value': f\"{p_val:.3f}\"\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Skipping t-test for {var} due to insufficient data in one or both groups.\")\n",
    "    else:\n",
    "        print(f\"Warning: Continuous covariate '{var}' not found in DataFrame.\")\n",
    "\n",
    "if balance_results_continuous_pre:\n",
    "    balance_df_continuous_pre = pd.DataFrame(balance_results_continuous_pre)\n",
    "    print(balance_df_continuous_pre)\n",
    "else:\n",
    "    print(\"No continuous covariates to check or insufficient data.\")\n",
    "\n",
    "\n",
    "# --- Balance for Binary Covariates ---\n",
    "binary_covariates = ['has_free_delivery', 'is_verified_seller', 'has_additional_discount_coupon']\n",
    "print(\"\\n--- Binary Covariate Balance Before Matching ---\")\n",
    "balance_results_binary_pre = []\n",
    "for var in binary_covariates:\n",
    "    if var in df.columns:\n",
    "        # Ensure there's data to compare\n",
    "        if not treated_group_pre_match[var].dropna().empty and not control_group_pre_match[var].dropna().empty:\n",
    "            prop_treated = treated_group_pre_match[var].mean() # Proportion of 1s\n",
    "            prop_control = control_group_pre_match[var].mean()\n",
    "            \n",
    "            count_treated = treated_group_pre_match[var].sum()\n",
    "            n_treated = treated_group_pre_match[var].count()\n",
    "            count_control = control_group_pre_match[var].sum()\n",
    "            n_control = control_group_pre_match[var].count()\n",
    "\n",
    "            if n_treated > 0 and n_control > 0: # Ensure non-zero observations for z-test\n",
    "                # Standardized Mean Difference (SMD) for binary variables\n",
    "                smd_binary = (prop_treated - prop_control) / np.sqrt((prop_treated * (1 - prop_treated) + prop_control * (1 - prop_control)) / 2)\n",
    "\n",
    "                z_stat, p_val = proportions_ztest([count_treated, count_control], [n_treated, n_control])\n",
    "                balance_results_binary_pre.append({\n",
    "                    'Covariate': var,\n",
    "                    'Prop Treated': f\"{prop_treated:.2f}\",\n",
    "                    'Prop Control': f\"{prop_control:.2f}\",\n",
    "                    'Std Diff (SMD)': f\"{smd_binary:.3f}\",\n",
    "                    'Z-Statistic': f\"{z_stat:.2f}\",\n",
    "                    'P-Value': f\"{p_val:.3f}\"\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Skipping z-test for {var} due to zero observations in one group.\")\n",
    "        else:\n",
    "            print(f\"Skipping balance check for {var} due to insufficient data.\")\n",
    "    else:\n",
    "        print(f\"Warning: Binary covariate '{var}' not found in DataFrame.\")\n",
    "\n",
    "if balance_results_binary_pre:\n",
    "    balance_df_binary_pre = pd.DataFrame(balance_results_binary_pre)\n",
    "    print(balance_df_binary_pre)\n",
    "else:\n",
    "    print(\"No binary covariates to check or insufficient data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf5a3d6",
   "metadata": {},
   "source": [
    "## Step 2: Run Naive Regression (Without Matching)\n",
    "\n",
    "We'll run a simple OLS regression to estimate the association between having a high discount and product star rating *before* matching, including all covariates. This provides a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c34848d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmatched regression formula: rating ~ is_high_discount + current_price + has_free_delivery + is_verified_seller + total_review_count + has_additional_discount_coupon\n",
      "\n",
      "--- Naive Regression (Unmatched Data) ---\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 rating   R-squared:                       0.147\n",
      "Model:                            OLS   Adj. R-squared:                  0.140\n",
      "Method:                 Least Squares   F-statistic:                     21.60\n",
      "Date:                Mon, 26 May 2025   Prob (F-statistic):           5.63e-20\n",
      "Time:                        16:48:14   Log-Likelihood:                -1384.6\n",
      "No. Observations:                 633   AIC:                             2781.\n",
      "Df Residuals:                     627   BIC:                             2808.\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==================================================================================================\n",
      "                                     coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Intercept                          2.0051      0.200     10.026      0.000       1.612       2.398\n",
      "is_high_discount                   0.0864      0.176      0.491      0.624      -0.259       0.432\n",
      "current_price                      0.0017      0.000      4.077      0.000       0.001       0.002\n",
      "has_free_delivery                 -1.2708      0.213     -5.971      0.000      -1.689      -0.853\n",
      "is_verified_seller                 1.5366      0.221      6.954      0.000       1.103       1.971\n",
      "total_review_count                 0.0025      0.001      4.653      0.000       0.001       0.004\n",
      "has_additional_discount_coupon          0          0        nan        nan           0           0\n",
      "==============================================================================\n",
      "Omnibus:                     6388.411   Durbin-Watson:                   1.920\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               59.541\n",
      "Skew:                           0.185   Prob(JB):                     1.18e-13\n",
      "Kurtosis:                       1.544   Cond. No.                          inf\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is      0. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/statsmodels/regression/linear_model.py:1965: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return np.sqrt(eigvals[0]/eigvals[-1])\n"
     ]
    }
   ],
   "source": [
    "# Construct the formula for regression\n",
    "# Ensure all covariates_list items are in df.columns\n",
    "valid_covariates_for_formula = [cov for cov in covariates_list if cov in df.columns]\n",
    "formula_unmatched = f\"{outcome_variable} ~ {treatment_variable} + {' + '.join(valid_covariates_for_formula)}\"\n",
    "print(f\"Unmatched regression formula: {formula_unmatched}\")\n",
    "\n",
    "try:\n",
    "    model_unmatched = sm.OLS.from_formula(formula_unmatched, data=df).fit()\n",
    "    print(\"\\n--- Naive Regression (Unmatched Data) ---\")\n",
    "    print(model_unmatched.summary())\n",
    "except Exception as e:\n",
    "    print(f\"Error running unmatched regression: {e}\")\n",
    "    print(\"This might be due to perfect multicollinearity or insufficient data/variation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c74d42e",
   "metadata": {},
   "source": [
    "## Step 3: Propensity Score Matching\n",
    "\n",
    "Now, we will perform Propensity Score Matching to create more comparable groups.\n",
    "\n",
    "1.  **Estimate Propensity Scores**: Use logistic regression to estimate the probability of a product having a 'High Discount' based on its covariates.\n",
    "2.  **Perform Matching**: Match each 'High Discount' product to one 'Low/No Discount' product with the closest propensity score (Nearest Neighbor Matching), potentially using a caliper to ensure match quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef4f7e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total units in matched sample: 562\n",
      "Number of treated units in matched sample: 281\n",
      "Number of control units in matched sample: 281\n"
     ]
    }
   ],
   "source": [
    "# Define covariates for propensity score model\n",
    "# Ensure all covariates_list items are in df.columns\n",
    "X_psm = df[valid_covariates_for_formula].copy() # Features for PSM\n",
    "y_psm = df[treatment_variable].copy()        # Treatment indicator\n",
    "\n",
    "# Handle potential NaNs in features for PSM (LogisticRegression doesn't like NaNs)\n",
    "# This should have been handled in data prep, but as a safeguard:\n",
    "X_psm.fillna(X_psm.median(), inplace=True) # Example: median imputation for numeric\n",
    "# For binary, mode imputation or careful handling is needed if not already done.\n",
    "# Ensure all covariates are numeric for sklearn's LogisticRegression (dummy encode if needed)\n",
    "\n",
    "# 1. Estimate propensity scores using logistic regression\n",
    "logit_model = LogisticRegression(solver='liblinear', max_iter=1000, class_weight='balanced') # Added class_weight for imbalanced treatment\n",
    "try:\n",
    "    logit_model.fit(X_psm, y_psm)\n",
    "    df['propensity_score'] = logit_model.predict_proba(X_psm)[:, 1]\n",
    "except ValueError as e:\n",
    "    print(f\"Error fitting Logistic Regression for propensity scores: {e}\")\n",
    "    print(\"Check for NaNs or non-numeric data in X_psm, or if a class in y_psm is missing.\")\n",
    "    # If error, create dummy propensity_score to allow rest of code to run for structure check\n",
    "    df['propensity_score'] = np.random.rand(len(df))\n",
    "\n",
    "\n",
    "# 2. Perform Matching\n",
    "treated_units = df[df[treatment_variable] == 1].copy()\n",
    "control_units = df[df[treatment_variable] == 0].copy()\n",
    "\n",
    "if treated_units.empty or control_units.empty or 'propensity_score' not in control_units.columns:\n",
    "    print(\"Cannot perform matching: Not enough data in treatment/control groups or propensity scores missing.\")\n",
    "    matched_df = pd.DataFrame() # Empty df if matching fails\n",
    "else:\n",
    "    # Ensure control_units for NearestNeighbors fit doesn't have NaNs in pscore\n",
    "    control_units_nn = control_units[['propensity_score']].copy()\n",
    "    control_units_nn.dropna(inplace=True) # Should be handled earlier, but safety\n",
    "    \n",
    "    if control_units_nn.empty:\n",
    "        print(\"Cannot perform matching: Control units have no valid propensity scores for KNN.\")\n",
    "        matched_df = pd.DataFrame()\n",
    "    else:\n",
    "        # Match treated units to control units\n",
    "        nn = NearestNeighbors(n_neighbors=1, algorithm='ball_tree')\n",
    "        nn.fit(control_units_nn[['propensity_score']])\n",
    "        \n",
    "        # Ensure treated_units for kneighbors doesn't have NaNs in pscore\n",
    "        treated_units_nn = treated_units[['propensity_score']].copy()\n",
    "        treated_units_nn.dropna(inplace=True) # Should be handled earlier\n",
    "        \n",
    "        if treated_units_nn.empty:\n",
    "            print(\"Cannot perform matching: Treated units have no valid propensity scores for KNN.\")\n",
    "            matched_df = pd.DataFrame()\n",
    "        else:\n",
    "            distances, indices = nn.kneighbors(treated_units_nn[['propensity_score']])\n",
    "\n",
    "            # Apply caliper to ensure match quality (optional but recommended)\n",
    "            CALIPER = 0.05 # Max allowed difference in propensity scores\n",
    "            matched_indices_in_control = indices.flatten()\n",
    "            \n",
    "            # Filter out matches outside the caliper\n",
    "            # Ensure indices are valid before trying to use them\n",
    "            valid_matches = []\n",
    "            original_treated_indices = treated_units_nn.index # Get original indices of treated units used for NN\n",
    "\n",
    "            for i in range(len(original_treated_indices)):\n",
    "                treated_idx = original_treated_indices[i]\n",
    "                control_original_idx = control_units_nn.index[matched_indices_in_control[i]] # Get original index of matched control\n",
    "                \n",
    "                if distances[i][0] <= CALIPER:\n",
    "                    valid_matches.append((treated_idx, control_original_idx))\n",
    "            \n",
    "            matched_treated_indices = [match[0] for match in valid_matches]\n",
    "            matched_control_indices = [match[1] for match in valid_matches]\n",
    "\n",
    "            df_matched_treated = df.loc[matched_treated_indices]\n",
    "            df_matched_control = df.loc[matched_control_indices]\n",
    "            \n",
    "            # Create the matched DataFrame\n",
    "            matched_df = pd.concat([df_matched_treated, df_matched_control])\n",
    "            print(f\"\\nTotal units in matched sample: {matched_df.shape[0]}\")\n",
    "            print(f\"Number of treated units in matched sample: {df_matched_treated.shape[0]}\")\n",
    "            print(f\"Number of control units in matched sample: {df_matched_control.shape[0]}\")\n",
    "\n",
    "if matched_df.empty:\n",
    "    print(\"Warning: Matched DataFrame is empty. Cannot proceed with post-match analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2152e44",
   "metadata": {},
   "source": [
    "## Step 4: Check Covariate Balance *After* Matching\n",
    "\n",
    "After matching, we re-evaluate the balance of covariates between the treated and the now-matched control groups. Ideally, any significant differences observed before matching should be substantially reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae8f324b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-match - Matched Treated group size: 281\n",
      "Post-match - Matched Control group size: 281\n",
      "\n",
      "--- Continuous Covariate Balance AFTER Matching ---\n",
      "            Covariate Mean Treated Mean Control Std Diff (SMD) T-Statistic  \\\n",
      "0       current_price       359.68       377.35         -0.088       -1.04   \n",
      "1  total_review_count        10.30         7.41          0.117        1.39   \n",
      "\n",
      "  P-Value  \n",
      "0   0.299  \n",
      "1   0.166  \n",
      "\n",
      "--- Binary Covariate Balance AFTER Matching ---\n",
      "                        Covariate Prop Treated Prop Control Std Diff (SMD)  \\\n",
      "0               has_free_delivery         0.60         0.60          0.000   \n",
      "1              is_verified_seller         0.20         0.19          0.027   \n",
      "2  has_additional_discount_coupon         0.00         0.00            nan   \n",
      "\n",
      "  Z-Statistic P-Value  \n",
      "0        0.00   1.000  \n",
      "1        0.32   0.749  \n",
      "2         nan     nan  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5p/6h8y8yzn7rxdgn4rkplx53rh0000gn/T/ipykernel_3270/722815390.py:64: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  smd_binary_post = (prop_treated_post - prop_control_post) / np.sqrt((prop_treated_post * (1 - prop_treated_post) + prop_control_post * (1 - prop_control_post)) / 2)\n",
      "/opt/anaconda3/lib/python3.11/site-packages/statsmodels/stats/weightstats.py:792: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  zstat = value / std\n"
     ]
    }
   ],
   "source": [
    "if not matched_df.empty:\n",
    "    # Separate matched treated and control groups\n",
    "    matched_treated_group = matched_df[matched_df[treatment_variable] == 1]\n",
    "    matched_control_group = matched_df[matched_df[treatment_variable] == 0]\n",
    "\n",
    "    if matched_treated_group.empty or matched_control_group.empty:\n",
    "        print(\"Not enough data in matched treatment or control group to check balance.\")\n",
    "    else:\n",
    "        print(f\"Post-match - Matched Treated group size: {len(matched_treated_group)}\")\n",
    "        print(f\"Post-match - Matched Control group size: {len(matched_control_group)}\")\n",
    "\n",
    "        # --- Balance for Continuous Covariates After Matching ---\n",
    "        print(\"\\n--- Continuous Covariate Balance AFTER Matching ---\")\n",
    "        balance_results_continuous_post = []\n",
    "        for var in continuous_covariates:\n",
    "            if var in matched_df.columns:\n",
    "                if not matched_treated_group[var].dropna().empty and not matched_control_group[var].dropna().empty:\n",
    "                    mean_treated_post = matched_treated_group[var].mean()\n",
    "                    mean_control_post = matched_control_group[var].mean()\n",
    "                    std_treated_post = matched_treated_group[var].std()\n",
    "                    std_control_post = matched_control_group[var].std()\n",
    "                    \n",
    "                    s_pooled_post = np.sqrt(((len(matched_treated_group)-1)*(std_treated_post**2) + \\\n",
    "                                            (len(matched_control_group)-1)*(std_control_post**2)) / \\\n",
    "                                        (len(matched_treated_group) + len(matched_control_group) - 2))\n",
    "                    smd_post = (mean_treated_post - mean_control_post) / s_pooled_post if s_pooled_post > 0 else 0\n",
    "\n",
    "                    t_stat_post, p_val_post = ttest_ind(matched_treated_group[var].dropna(), matched_control_group[var].dropna(), equal_var=False)\n",
    "                    balance_results_continuous_post.append({\n",
    "                        'Covariate': var,\n",
    "                        'Mean Treated': f\"{mean_treated_post:.2f}\",\n",
    "                        'Mean Control': f\"{mean_control_post:.2f}\",\n",
    "                        'Std Diff (SMD)': f\"{smd_post:.3f}\",\n",
    "                        'T-Statistic': f\"{t_stat_post:.2f}\",\n",
    "                        'P-Value': f\"{p_val_post:.3f}\"\n",
    "                    })\n",
    "                else:\n",
    "                    print(f\"Skipping post-match t-test for {var} due to insufficient data.\")\n",
    "            else:\n",
    "                print(f\"Warning: Post-match continuous covariate '{var}' not found.\")\n",
    "        \n",
    "        if balance_results_continuous_post:\n",
    "            balance_df_continuous_post = pd.DataFrame(balance_results_continuous_post)\n",
    "            print(balance_df_continuous_post)\n",
    "        else:\n",
    "            print(\"No continuous covariates to check post-match or insufficient data.\")\n",
    "\n",
    "\n",
    "        # --- Balance for Binary Covariates After Matching ---\n",
    "        print(\"\\n--- Binary Covariate Balance AFTER Matching ---\")\n",
    "        balance_results_binary_post = []\n",
    "        for var in binary_covariates:\n",
    "            if var in matched_df.columns:\n",
    "                if not matched_treated_group[var].dropna().empty and not matched_control_group[var].dropna().empty:\n",
    "                    prop_treated_post = matched_treated_group[var].mean()\n",
    "                    prop_control_post = matched_control_group[var].mean()\n",
    "                    \n",
    "                    count_treated_post = matched_treated_group[var].sum()\n",
    "                    n_treated_post = matched_treated_group[var].count()\n",
    "                    count_control_post = matched_control_group[var].sum()\n",
    "                    n_control_post = matched_control_group[var].count()\n",
    "\n",
    "                    if n_treated_post > 0 and n_control_post > 0:\n",
    "                        smd_binary_post = (prop_treated_post - prop_control_post) / np.sqrt((prop_treated_post * (1 - prop_treated_post) + prop_control_post * (1 - prop_control_post)) / 2)\n",
    "                        \n",
    "                        z_stat_post, p_val_post = proportions_ztest([count_treated_post, count_control_post], [n_treated_post, n_control_post])\n",
    "                        balance_results_binary_post.append({\n",
    "                            'Covariate': var,\n",
    "                            'Prop Treated': f\"{prop_treated_post:.2f}\",\n",
    "                            'Prop Control': f\"{prop_control_post:.2f}\",\n",
    "                            'Std Diff (SMD)': f\"{smd_binary_post:.3f}\",\n",
    "                            'Z-Statistic': f\"{z_stat_post:.2f}\",\n",
    "                            'P-Value': f\"{p_val_post:.3f}\"\n",
    "                        })\n",
    "                    else:\n",
    "                        print(f\"Skipping post-match z-test for {var} due to zero observations.\")\n",
    "                else:\n",
    "                    print(f\"Skipping post-match balance for {var} due to insufficient data.\")\n",
    "            else:\n",
    "                print(f\"Warning: Post-match binary covariate '{var}' not found.\")\n",
    "\n",
    "        if balance_results_binary_post:\n",
    "            balance_df_binary_post = pd.DataFrame(balance_results_binary_post)\n",
    "            print(balance_df_binary_post)\n",
    "        else:\n",
    "            print(\"No binary covariates to check post-match or insufficient data.\")\n",
    "else:\n",
    "    print(\"Skipping post-match balance checks as matched_df is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b6fa1f",
   "metadata": {},
   "source": [
    "## Step 5: Estimate Treatment Effect on Matched Sample\n",
    "\n",
    "With the matched sample, we can now estimate the effect of 'High Discount' on 'Product Star Rating'. We can do this by comparing the means of the outcome variable between the treated and matched control groups, or by running a regression on the matched sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e98cf782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean rating for Matched Treated (High Discount) group: 2.284\n",
      "Mean rating for Matched Control (Low/No Discount) group: 2.353\n",
      "T-test for difference in rating between matched groups:\n",
      "T-Statistic: -0.35, P-Value: 0.726\n",
      "Estimated Average Treatment Effect (ATT) on rating: -0.070\n",
      "\n",
      "Matched regression formula: rating ~ is_high_discount + current_price + has_free_delivery + is_verified_seller + total_review_count + has_additional_discount_coupon\n",
      "\n",
      "--- Regression on Matched Data ---\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 rating   R-squared:                       0.190\n",
      "Model:                            OLS   Adj. R-squared:                  0.183\n",
      "Method:                 Least Squares   F-statistic:                     26.08\n",
      "Date:                Mon, 26 May 2025   Prob (F-statistic):           1.08e-23\n",
      "Time:                        16:48:31   Log-Likelihood:                -1218.7\n",
      "No. Observations:                 562   AIC:                             2449.\n",
      "Df Residuals:                     556   BIC:                             2475.\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==================================================================================================\n",
      "                                     coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "Intercept                          2.1019      0.213      9.869      0.000       1.684       2.520\n",
      "is_high_discount                  -0.1615      0.180     -0.896      0.370      -0.515       0.192\n",
      "current_price                      0.0007      0.001      1.187      0.236      -0.000       0.002\n",
      "has_free_delivery                 -0.8522      0.249     -3.418      0.001      -1.342      -0.362\n",
      "is_verified_seller                 1.4102      0.230      6.128      0.000       0.958       1.862\n",
      "total_review_count                 0.0309      0.004      8.474      0.000       0.024       0.038\n",
      "has_additional_discount_coupon          0          0        nan        nan           0           0\n",
      "==============================================================================\n",
      "Omnibus:                     6014.796   Durbin-Watson:                   2.078\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               55.682\n",
      "Skew:                           0.253   Prob(JB):                     8.11e-13\n",
      "Kurtosis:                       1.544   Cond. No.                          inf\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is      0. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n",
      "\n",
      "--- Comparison of Treatment Effects ---\n",
      "Unmatched model 'is_high_discount' Coef: 0.0864, P-value: 0.624\n",
      "Matched model 'is_high_discount' Coef: -0.1615, P-value: 0.370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/statsmodels/regression/linear_model.py:1965: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return np.sqrt(eigvals[0]/eigvals[-1])\n"
     ]
    }
   ],
   "source": [
    "if not matched_df.empty and not matched_treated_group.empty and not matched_control_group.empty:\n",
    "    # Method 1: Direct comparison of means for the outcome variable\n",
    "    mean_rating_treated = matched_treated_group[outcome_variable].mean()\n",
    "    mean_rating_control = matched_control_group[outcome_variable].mean()\n",
    "    print(f\"\\nMean {outcome_variable} for Matched Treated (High Discount) group: {mean_rating_treated:.3f}\")\n",
    "    print(f\"Mean {outcome_variable} for Matched Control (Low/No Discount) group: {mean_rating_control:.3f}\")\n",
    "    \n",
    "    # Perform t-test for difference in means on the outcome\n",
    "    t_stat_outcome, p_val_outcome = ttest_ind(\n",
    "        matched_treated_group[outcome_variable].dropna(),\n",
    "        matched_control_group[outcome_variable].dropna(),\n",
    "        equal_var=False # Assuming unequal variances is safer\n",
    "    )\n",
    "    print(f\"T-test for difference in {outcome_variable} between matched groups:\")\n",
    "    print(f\"T-Statistic: {t_stat_outcome:.2f}, P-Value: {p_val_outcome:.3f}\")\n",
    "    print(f\"Estimated Average Treatment Effect (ATT) on {outcome_variable}: {mean_rating_treated - mean_rating_control:.3f}\")\n",
    "\n",
    "    # Method 2: Regression on the matched sample\n",
    "    # This can help control for any residual imbalances in covariates after matching\n",
    "    formula_matched = f\"{outcome_variable} ~ {treatment_variable} + {' + '.join(valid_covariates_for_formula)}\"\n",
    "    print(f\"\\nMatched regression formula: {formula_matched}\")\n",
    "    \n",
    "    try:\n",
    "        model_matched = sm.OLS.from_formula(formula_matched, data=matched_df).fit()\n",
    "        print(\"\\n--- Regression on Matched Data ---\")\n",
    "        print(model_matched.summary())\n",
    "        \n",
    "        # Compare treatment effects\n",
    "        print(\"\\n--- Comparison of Treatment Effects ---\")\n",
    "        if 'model_unmatched' in locals():\n",
    "            print(f\"Unmatched model '{treatment_variable}' Coef: {model_unmatched.params.get(treatment_variable, 'N/A'):.4f}, P-value: {model_unmatched.pvalues.get(treatment_variable, 'N/A'):.3f}\")\n",
    "        else:\n",
    "            print(\"Unmatched model was not successfully run or 'treat' variable not found.\")\n",
    "\n",
    "        print(f\"Matched model '{treatment_variable}' Coef: {model_matched.params.get(treatment_variable, 'N/A'):.4f}, P-value: {model_matched.pvalues.get(treatment_variable, 'N/A'):.3f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error running matched regression: {e}\")\n",
    "        print(\"This might be due to perfect multicollinearity or insufficient data/variation in the matched sample.\")\n",
    "else:\n",
    "    print(\"Cannot estimate treatment effect as matched_df or matched groups are empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82117e07",
   "metadata": {},
   "source": [
    "## Step 6: Interpretation and Conclusion\n",
    "\n",
    "**Summarize Findings:**\n",
    "* Report the estimated effect of `is_high_discount` on `product_star_rating` from the matched analysis.\n",
    "* State whether this effect is statistically significant.\n",
    "* Compare this to the naive (unmatched) estimate if calculated.\n",
    "\n",
    "**Discuss Limitations:**\n",
    "* **Data Availability for Covariates**: Explicitly state which of the desired covariates (`has_free_delivery`, `is_verified_seller`, `has_additional_discount_coupon`) were actually available and used. If any were missing, this is a key limitation as they could be important confounders.\n",
    "* **Discount Data Reliability**: Discuss any challenges in defining or calculating `discount_percentage` and `is_high_discount` (e.g., issues with `old_price`).\n",
    "* **Unobserved Confounders**: PSM only controls for *observed* covariates. There might be other unobserved factors that influence both a product's discount strategy and its star rating (e.g., product quality not captured by covariates, seller's marketing effort, specific inventory levels).\n",
    "* **Cross-Sectional Data**: This analysis is based on a snapshot. We cannot infer changes over time or the precise timing of when discounts were applied relative to when ratings were given (beyond what the overall rating represents).\n",
    "* **Category Specificity**: Results are specific to the chosen product category and may not generalize.\n",
    "* **Small Sample Size**: If your final matched sample is small, the estimates might be imprecise or unstable.\n",
    "\n",
    "**Conclusion**:\n",
    "Based on the analysis, provide a tentative conclusion about the relationship between offering a high discount and product star ratings for the specific category analyzed, keeping all limitations in mind. For example: \"Within the [Your Category] category, after attempting to control for [list your covariates], products with a high discount percentage were found to have, on average, [X points higher/lower/no significant difference in] star ratings compared to matched products with lower/no discounts. However, this finding should be interpreted with caution due to [mention key limitations].\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c33bdf5",
   "metadata": {},
   "source": [
    "## Part 3: Prescriptive Analytics with a Large Language Model (LLM) ð¤\n",
    "\n",
    "This section outlines how we will use the OpenAI API to interact with a Large Language Model (LLM) like GPT-3.5-turbo or GPT-4. The goal is to generate actionable, prescriptive recommendations based on the diagnostic findings from our causal inference analysis (Part 2 of the assignment). We aim to address a specific challenge identified from our results, fulfilling the \"Basic Expectation\" and touching upon the \"Additional Criteria\" for Part 3 of the assignment.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Configuration and API Key Setup ð\n",
    "\n",
    "First, we need to import the necessary `openai` library and configure our API key.\n",
    "\n",
    "**Security Note**: Your OpenAI API key is sensitive. It's crucial to manage it securely. In a production or shared environment, you should use environment variables or a secure configuration file instead of hardcoding the key directly in the notebook. For this assignment, ensure the key is not publicly shared (e.g., if you push your code to a public repository).\n",
    "\n",
    "```python\n",
    "import openai\n",
    "import pandas as pd # pandas might be useful for handling data summaries if needed\n",
    "\n",
    "# --- OpenAI API Key Configuration ---\n",
    "# Replace \"YOUR_OPENAI_API_KEY\" with your actual key before running.\n",
    "# It's strongly recommended to load it from an environment variable or a local config file\n",
    "# for better security, e.g., using:\n",
    "# import os\n",
    "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# If you haven't set it as an environment variable, you can temporarily set it here:\n",
    "\n",
    "try:\n",
    "    # PASTE YOUR ACTUAL OPENAI API KEY HERE\n",
    "    openai_api_key = \"YOUR_OPENAI_API_KEY\" \n",
    "    if not openai_api_key or openai_api_key == \"YOUR_OPENAI_API_KEY\":\n",
    "        raise ValueError(\"API key not set or still a placeholder. Please provide your actual OpenAI API key.\")\n",
    "    openai.api_key = openai_api_key\n",
    "    print(\"OpenAI API key configured.\")\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"LLM interaction will not work without a valid API key.\")\n",
    "    # Provide a dummy key to allow the notebook to run for structure review, but API calls will fail.\n",
    "    openai.api_key = \"sk-dummykeyforstructuretestingxxxxxxxx\"\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during API key configuration: {e}\")\n",
    "    openai.api_key = \"sk-dummykeyforstructuretestingxxxxxxxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3905b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: Securely manage your API key. Do not commit it to version control.\n",
    "# Consider using environment variables or a config file.\n",
    "# For this example, we'll prompt, but in a real script, use a secure method.\n",
    "try:\n",
    "    openai.api_key = \"YOUR_OPENAI_API_KEY\" # Replace with your actual key\n",
    "    if openai.api_key == \"YOUR_OPENAI_API_KEY\":\n",
    "        raise ValueError(\"Please replace 'YOUR_OPENAI_API_KEY' with your actual OpenAI API key.\")\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "    # Fallback for a placeholder if you don't want to put a real key here during initial drafting\n",
    "    # The API call will fail if this placeholder is used.\n",
    "    openai.api_key = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n",
    "\n",
    "\n",
    "# --- 1. Summarize Your Diagnostic Findings (from Part 2) ---\n",
    "# Example based on Scenario 1 (High Discount Negatively Affects Rating)\n",
    "# REPLACE with your actual findings and category\n",
    "chosen_category = \"Smartphones\" # << REPLACE\n",
    "diagnostic_summary = f\"\"\"\n",
    "Our causal inference analysis (using Propensity Score Matching) on MercadoLibre products\n",
    "in the '{chosen_category}' category investigated the effect of 'high discount percentages'\n",
    "(defined as >25%) on 'product star ratings' (1-5 stars).\n",
    "Covariates controlled for included: current price, free delivery status,\n",
    "verified seller status, total review count, and presence of additional discount coupons.\n",
    "\n",
    "Key Finding: We found a statistically significant negative effect.\n",
    "Products with high discounts had, on average, a {0.35:.2f} point lower star rating\n",
    "(e.g., average rating of 4.15 for high discount group vs. 4.50 for matched low/no discount group, p < 0.05)\n",
    "compared to similar products with low or no discounts.\n",
    "This suggests that while discounts might drive sales, they could be harming perceived product quality\n",
    "or customer satisfaction post-purchase in this category.\n",
    "\"\"\"\n",
    "# << REPLACE THE FINDING DETAILS (0.35 points, 4.15 vs 4.50, p-value) WITH YOUR ACTUAL RESULTS\n",
    "\n",
    "# --- 2. Define Your Prescriptive Problem ---\n",
    "# Example based on Scenario 1\n",
    "prescriptive_question = f\"\"\"\n",
    "Given our finding that high discounts negatively impact star ratings for '{chosen_category}'\n",
    "on MercadoLibre, what specific, actionable strategies can sellers of '{chosen_category}'\n",
    "implement to mitigate this negative impact on ratings when they utilize high discount strategies?\n",
    "We are looking for recommendations related to product presentation, communication with buyers,\n",
    "post-purchase engagement, and ways to frame discounts that might preserve or enhance perceived value.\n",
    "Consider that data insights are based on listings' current price, discount visibility, seller status,\n",
    "free delivery, additional coupons, and overall rating/review count.\n",
    "\"\"\"\n",
    "\n",
    "# --- 3. Prepare Context from Your Collected Data (Brief Summary) ---\n",
    "# This part is about giving the LLM a bit more context.\n",
    "# You won't \"train\" it on the raw CSV in a few API calls.\n",
    "# Instead, provide aggregated insights or typical product profiles.\n",
    "\n",
    "# Example:\n",
    "data_context_summary = f\"\"\"\n",
    "Products in the '{chosen_category}' category on MercadoLibre typically range\n",
    "from $150 to $1200. Discounts, when applied, can be substantial (10-50%).\n",
    "Many listings offer free delivery. Both verified and unverified sellers operate.\n",
    "Review counts vary widely. Additional coupons are sometimes present.\n",
    "Customers value [e.g., battery life, camera quality, screen resolution - IF YOU KNOW THIS FROM YOUR DATA/EXPLORATION].\n",
    "The platform allows sellers to have detailed product descriptions and images.\n",
    "\"\"\"\n",
    "# << CUSTOMIZE THIS CONTEXT BASED ON YOUR EXPLORATORY DATA ANALYSIS\n",
    "\n",
    "# --- 4. Craft the Prompt for the LLM ---\n",
    "prompt = f\"\"\"\n",
    "**Background & Diagnostic Findings:**\n",
    "{diagnostic_summary}\n",
    "\n",
    "**Data Context for '{chosen_category}' on MercadoLibre:**\n",
    "{data_context_summary}\n",
    "\n",
    "**Prescriptive Analytics Question:**\n",
    "{prescriptive_question}\n",
    "\n",
    "Please provide a set of 3-5 concrete, actionable recommendations for sellers.\n",
    "For each recommendation, briefly explain the rationale and how it addresses the problem.\n",
    "Focus on strategies that sellers can realistically implement.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Sending Prompt to LLM ---\")\n",
    "print(prompt[:500] + \"...\\n[Prompt Truncated for Display]\") # Print a part of the prompt\n",
    "\n",
    "# --- 5. Use Python to Connect to an LLM (OpenAI API Call) ---\n",
    "# Ensure you have the 'openai' library installed: pip install openai\n",
    "# This is a conceptual example. Refer to the latest OpenAI API documentation for exact usage.\n",
    "try:\n",
    "    # Note: The 'chat.completions.create' is the modern way for GPT-3.5-turbo and GPT-4\n",
    "    # Ensure your openai library is up-to-date (pip install --upgrade openai)\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", # Or \"gpt-4\" if you have access and prefer it\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert e-commerce strategy consultant providing prescriptive analytics advice.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=500,  # Adjust as needed\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7, # Adjust for more creative (higher) or factual (lower) responses\n",
    "    )\n",
    "    \n",
    "    llm_recommendations = response.choices[0].message.content.strip()\n",
    "    \n",
    "    print(\"\\n--- LLM Generated Prescriptive Recommendations ---\")\n",
    "    print(llm_recommendations)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred with the OpenAI API call: {e}\")\n",
    "    print(\"Please ensure your API key is correct and you have credits.\")\n",
    "    llm_recommendations = \"Error generating recommendations. Please check API key and connection.\"\n",
    "\n",
    "# --- 6. (For your report) Summary of LLM's Recommendations and Team's Reflection ---\n",
    "# This part you'll write based on the output.\n",
    "# e.g., \"The LLM suggested focusing on X, Y, Z. We found these recommendations insightful because...\n",
    "# However, recommendation Y might be challenging to implement due to...\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
