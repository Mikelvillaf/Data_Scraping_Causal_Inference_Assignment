{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0289eb64",
   "metadata": {},
   "source": [
    "# Section 1: Data Loading and Initial Exploration\n",
    "\n",
    "This section covers the initial steps of our analysis. We'll import the pandas library for data manipulation and then define a function to load our dataset, which is `review_data.csv`. After loading, we'll perform a basic exploratory data analysis (EDA) by displaying the first few rows, a summary of data types and non-null counts, and descriptive statistics to get a quick overview of the data's structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2dcfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_and_explore_data(file_path: str):\n",
    "    \"\"\"Loads data and performs a basic exploration.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"--- First 5 Rows ---\")\n",
    "    print(df.head())\n",
    "    print(\"\\n--- Data Info ---\")\n",
    "    df.info()\n",
    "    print(\"\\n--- Descriptive Statistics ---\")\n",
    "    print(df.describe(include='all'))\n",
    "    return df\n",
    "\n",
    "# Example usage for this section (will be integrated into main later)\n",
    "# review_df = load_and_explore_data('review_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe750db",
   "metadata": {},
   "source": [
    "# Section 2: Data Preprocessing\n",
    "\n",
    "In this section, we'll focus on preparing the data for feature engineering. This involves:\n",
    "1.  **Timestamp Conversion**: Converting the `timestamp` column to datetime objects, which is essential for time-based calculations like review age.\n",
    "2.  **Price Conversion**: Ensuring the `price` column is a numeric type. This might involve removing currency symbols or other non-numeric characters.\n",
    "3.  **Text Cleaning (Basic)**: Converting `review_text` to lowercase and stripping leading/trailing whitespace as a foundational step for text analysis.\n",
    "4.  **Handling Missing Values**: For simplicity in this draft, we will fill missing numerical values with the median and missing text/categorical values with a placeholder like \"Unknown\" or an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2861a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df: pd.DataFrame):\n",
    "    \"\"\"Preprocesses the dataframe: handles data types and basic cleaning.\"\"\"\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # Convert timestamp\n",
    "    if 'timestamp' in df_processed.columns:\n",
    "        df_processed['timestamp'] = pd.to_datetime(df_processed['timestamp'], errors='coerce')\n",
    "\n",
    "    # Convert price to numeric\n",
    "    if 'price' in df_processed.columns:\n",
    "        if df_processed['price'].dtype == 'object':\n",
    "            df_processed['price'] = df_processed['price'].astype(str).str.replace(r'[^\\d\\.]', '', regex=True)\n",
    "            df_processed['price'] = pd.to_numeric(df_processed['price'], errors='coerce')\n",
    "\n",
    "    # Basic text cleaning\n",
    "    if 'review_text' in df_processed.columns:\n",
    "        df_processed['review_text'] = df_processed['review_text'].astype(str).str.lower().str.strip()\n",
    "\n",
    "    # Handle missing values\n",
    "    for col in df_processed.select_dtypes(include=np.number).columns:\n",
    "        df_processed[col] = df_processed[col].fillna(df_processed[col].median())\n",
    "    for col in df_processed.select_dtypes(include='object').columns:\n",
    "        # A more robust strategy might be needed per column\n",
    "        df_processed[col] = df_processed[col].fillna('Unknown')\n",
    "    if 'review_text' in df_processed.columns and df_processed['review_text'].dtype == 'object':\n",
    "        df_processed['review_text'] = df_processed['review_text'].replace('Unknown','')\n",
    "\n",
    "\n",
    "    # Drop rows where critical conversions failed (e.g., timestamp to NaT)\n",
    "    df_processed.dropna(subset=['timestamp', 'price'], inplace=True)\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "# Example usage:\n",
    "# if 'review_df' in globals():\n",
    "#     preprocessed_df = preprocess_data(review_df)\n",
    "#     print(\"\\n--- Preprocessed Data Info ---\")\n",
    "#     preprocessed_df.info()\n",
    "#     print(preprocessed_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6e40f8",
   "metadata": {},
   "source": [
    "# Section 3: Feature Engineering\n",
    "\n",
    "This section is dedicated to creating the features required for our Propensity Score Matching (PSM) analysis:\n",
    "1.  **Treatment Variable (X)**: Create a binary column `mention_shipping`. It will be 1 if keywords like \"shipping,\" \"delivery,\" or \"entrega\" are found in `review_text`, and 0 otherwise.\n",
    "2.  **Review Age**: Calculate `review_age_days` from the `timestamp`, possibly relative to the most recent review date.\n",
    "3.  **Review Length**: Calculate `review_length` as the number of characters in the `review_text`.\n",
    "4.  **Sentiment Score (Basic Placeholder)**: A very simple rule-based sentiment score (`sentiment_score_basic`) will be generated. *A more sophisticated NLP approach is recommended for the actual project.*\n",
    "5.  **Product Category Encoding**: The `product_category` column, being categorical, will be one-hot encoded to be used as a covariate in the propensity score model.\n",
    "\n",
    "The outcome variable `star_rating` is already present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f56f9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "SHIPPING_KEYWORDS = [\n",
    "    'shipping', 'shipment', 'shipped', 'delivery', 'delivered', 'delivering', 'entrega',\n",
    "    'courier', 'carrier', 'fulfillment', 'dispatch', 'postage', 'freight'\n",
    "]\n",
    "\n",
    "def engineer_features(df: pd.DataFrame):\n",
    "    \"\"\"Engineers features for PSM.\"\"\"\n",
    "    df_featured = df.copy()\n",
    "\n",
    "    # 1. Treatment Variable\n",
    "    keyword_pattern = r'\\b(' + '|'.join(SHIPPING_KEYWORDS) + r')\\b'\n",
    "    df_featured['mention_shipping'] = df_featured['review_text'].str.contains(keyword_pattern, case=False, regex=True).astype(int)\n",
    "\n",
    "    # 2. Review Age\n",
    "    if 'timestamp' in df_featured.columns and not df_featured['timestamp'].empty:\n",
    "        most_recent_date = df_featured['timestamp'].max()\n",
    "        df_featured['review_age_days'] = (most_recent_date - df_featured['timestamp']).dt.days\n",
    "    else:\n",
    "        df_featured['review_age_days'] = 0 # Fallback\n",
    "\n",
    "    # 3. Review Length\n",
    "    df_featured['review_length'] = df_featured['review_text'].apply(lambda x: len(str(x)))\n",
    "\n",
    "    # 4. Sentiment Score (Basic Placeholder)\n",
    "    # This is a very simplistic approach.\n",
    "    positive_keywords = ['good', 'great', 'excellent', 'love', 'happy', 'satisfied', 'amazing', 'perfect']\n",
    "    negative_keywords = ['bad', 'poor', 'terrible', 'hate', 'unhappy', 'disappointed', 'worst', 'awful']\n",
    "    def basic_sentiment(text):\n",
    "        score = 0\n",
    "        for kw in positive_keywords: score += text.count(kw)\n",
    "        for kw in negative_keywords: score -= text.count(kw)\n",
    "        if score > 0: return 1\n",
    "        if score < 0: return -1\n",
    "        return 0\n",
    "    df_featured['sentiment_score_basic'] = df_featured['review_text'].apply(basic_sentiment)\n",
    "\n",
    "    # 5. Product Category Encoding\n",
    "    if 'product_category' in df_featured.columns:\n",
    "        df_featured = pd.get_dummies(df_featured, columns=['product_category'], prefix='cat', dummy_na=False)\n",
    "    \n",
    "    # Ensure 'price' exists as it's a key covariate\n",
    "    if 'price' not in df_featured.columns:\n",
    "        df_featured['price'] = 0.0 # Fallback, real data should have it\n",
    "\n",
    "    return df_featured\n",
    "\n",
    "# Example usage:\n",
    "# if 'preprocessed_df' in globals():\n",
    "#     featured_df = engineer_features(preprocessed_df)\n",
    "#     print(\"\\n--- Featured Data Info ---\")\n",
    "#     featured_df.info()\n",
    "#     print(featured_df[['review_id', 'star_rating', 'price', 'mention_shipping', 'review_age_days', 'review_length', 'sentiment_score_basic'] + [col for col in featured_df if col.startswith('cat_')]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3461cef8",
   "metadata": {},
   "source": [
    "# Section 4: Data Splitting\n",
    "\n",
    "Here, we'll split our dataset into training, validation, and test sets. This is crucial for training our propensity score model and then evaluating its performance and the subsequent causal effect estimation on unseen data. We'll use `train_test_split` from `sklearn.model_selection`. Stratification based on the treatment variable (`mention_shipping`) can help ensure similar distributions of treated and control units across the splits, if the data allows. The covariates (`X_psm`) will be features like `price`, `review_age_days`, `review_length`, `sentiment_score_basic`, and the one-hot encoded `product_category` columns. The treatment indicator is `T`, and the outcome is `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b023bcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(df: pd.DataFrame, treatment_col: str, outcome_col: str, base_covariates: list, test_size=0.2, val_size=0.125, random_state=42):\n",
    "    \"\"\"Splits data into training, validation, and test sets.\"\"\"\n",
    "    \n",
    "    # Identify actual covariate columns after one-hot encoding product_category\n",
    "    # and ensure only existing columns are used.\n",
    "    encoded_covariates = [c for c in df.columns if c.startswith('cat_')]\n",
    "    \n",
    "    # Combine base covariates (excluding original product_category if it was one-hot encoded)\n",
    "    # with the new one-hot encoded columns.\n",
    "    final_covariate_list = [cov for cov in base_covariates if cov != 'product_category' and cov in df.columns]\n",
    "    final_covariate_list.extend(encoded_covariates)\n",
    "    \n",
    "    # Ensure essential columns are present\n",
    "    missing_cols = [col for col in [treatment_col, outcome_col] + final_covariate_list if col not in df.columns]\n",
    "    if any(col not in df.columns for col in [treatment_col, outcome_col]):\n",
    "        raise ValueError(f\"Treatment ('{treatment_col}') or outcome ('{outcome_col}') column missing from DataFrame.\")\n",
    "    if not final_covariate_list:\n",
    "        raise ValueError(\"No covariate columns found in DataFrame.\")\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Some covariate columns missing, proceeding with available: {missing_cols}\")\n",
    "        final_covariate_list = [c for c in final_covariate_list if c in df.columns]\n",
    "\n",
    "\n",
    "    X_psm = df[final_covariate_list]\n",
    "    T = df[treatment_col]\n",
    "    Y = df[outcome_col]\n",
    "\n",
    "    # First split: training and temporary (validation + test)\n",
    "    X_train_psm, X_temp_psm, T_train, T_temp, Y_train, Y_temp = train_test_split(\n",
    "        X_psm, T, Y, test_size=(test_size + (val_size * (1-test_size))), # val_size is % of original, adjusted here\n",
    "        random_state=random_state, stratify=T if T.nunique() > 1 else None\n",
    "    )\n",
    "\n",
    "    # Second split: validation and test from temporary\n",
    "    # val_size here is proportion of the temp set\n",
    "    actual_val_size_for_temp = val_size / (test_size + val_size) if (test_size + val_size) > 0 else 0\n",
    "    if X_temp_psm.empty or actual_val_size_for_temp == 0 or actual_val_size_for_temp >= 1: # handle edge case of no val/test desired or rounding\n",
    "        X_val_psm, T_val, Y_val = pd.DataFrame(), pd.Series(dtype=T_train.dtype), pd.Series(dtype=Y_train.dtype)\n",
    "        X_test_psm, T_test, Y_test = X_temp_psm, T_temp, Y_temp\n",
    "    else:\n",
    "        X_val_psm, X_test_psm, T_val, T_test, Y_val, Y_test = train_test_split(\n",
    "            X_temp_psm, T_temp, Y_temp, test_size= (1-actual_val_size_for_temp), # test is remainder\n",
    "            random_state=random_state, stratify=T_temp if T_temp.nunique() > 1 else None\n",
    "        )\n",
    "    \n",
    "    return X_train_psm, T_train, Y_train, X_val_psm, T_val, Y_val, X_test_psm, T_test, Y_test\n",
    "\n",
    "# Example usage:\n",
    "# if 'featured_df' in globals():\n",
    "#     COVARIATES_FOR_PSM = ['price', 'review_age_days', 'review_length', 'sentiment_score_basic', 'product_category'] # Base name\n",
    "#     TREATMENT = 'mention_shipping'\n",
    "#     OUTCOME = 'star_rating'\n",
    "#     splits = split_data(featured_df, TREATMENT, OUTCOME, COVARIATES_FOR_PSM)\n",
    "#     X_train, T_train, Y_train, X_val, T_val, Y_val, X_test, T_test, Y_test = splits\n",
    "#     print(f\"Train shapes: X-{X_train.shape}, T-{T_train.shape}, Y-{Y_train.shape}\")\n",
    "#     if not X_val.empty: print(f\"Val shapes: X-{X_val.shape}, T-{T_val.shape}, Y-{Y_val.shape}\")\n",
    "#     print(f\"Test shapes: X-{X_test.shape}, T-{T_test.shape}, Y-{Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7479675",
   "metadata": {},
   "source": [
    "# Section 5: Propensity Score Model Training\n",
    "\n",
    "This section details the training of the propensity score model. We will use Logistic Regression to predict the probability of a review mentioning shipping (the treatment), given the set of covariates derived in Section 3. Steps include:\n",
    "1.  **Scaling Covariates**: Numerical covariates will be standardized (scaled) for better model performance.\n",
    "2.  **Training**: A Logistic Regression model is trained on the training set (`X_train_psm`, `T_train`).\n",
    "3.  **Prediction**: Propensity scores (probabilities) are then predicted for the training, validation, and test sets.\n",
    "Optionally, we can plot the distribution of these scores for the treated and control groups to visually inspect overlap, which is critical for PSM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061f7b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def train_propensity_model(X_train: pd.DataFrame, T_train: pd.Series, \n",
    "                        X_val: pd.DataFrame = None, X_test: pd.DataFrame = None):\n",
    "    \"\"\"Trains a logistic regression model for propensity scores and predicts scores.\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    \n",
    "    model = LogisticRegression(solver='liblinear', random_state=42, class_weight='balanced')\n",
    "    model.fit(X_train_scaled, T_train)\n",
    "\n",
    "    ps_train = model.predict_proba(X_train_scaled)[:, 1]\n",
    "    ps_val = None\n",
    "    if X_val is not None and not X_val.empty:\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        ps_val = model.predict_proba(X_val_scaled)[:, 1]\n",
    "    \n",
    "    ps_test = None\n",
    "    if X_test is not None and not X_test.empty:\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        ps_test = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        \n",
    "    return model, ps_train, ps_val, ps_test, scaler\n",
    "\n",
    "\n",
    "def plot_propensity_score_overlap(ps_scores: np.ndarray, treatment_indicator: pd.Series, title: str):\n",
    "    \"\"\"Visualizes propensity score distribution for overlap assessment.\"\"\"\n",
    "    if ps_scores is None or treatment_indicator is None: return\n",
    "    \n",
    "    df_plot = pd.DataFrame({'propensity_score': ps_scores, 'treatment': treatment_indicator})\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.histplot(data=df_plot, x='propensity_score', hue='treatment', kde=True, stat=\"density\", common_norm=False)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Propensity Score')\n",
    "    plt.ylabel('Density')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# if 'X_train' in globals() and not X_train.empty:\n",
    "#     ps_model_trained, prop_scores_train, prop_scores_val, prop_scores_test, fitted_scaler = train_propensity_model(X_train, T_train, X_val, X_test)\n",
    "#     print(\"Propensity score model trained.\")\n",
    "#     if prop_scores_train is not None:\n",
    "#         plot_propensity_score_overlap(prop_scores_train, T_train, \"Propensity Score Overlap (Training Set)\")\n",
    "#     if prop_scores_val is not None and not T_val.empty:\n",
    "#         plot_propensity_score_overlap(prop_scores_val, T_val, \"Propensity Score Overlap (Validation Set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0d78e2",
   "metadata": {},
   "source": [
    "# Section 6: Main Execution and PSM Application (Conceptual Outline)\n",
    "\n",
    "This final section brings together all the preceding functions into a main pipeline. It will:\n",
    "1.  Load and preprocess the data.\n",
    "2.  Engineer the necessary features.\n",
    "3.  Split the data into training, validation, and test sets.\n",
    "4.  Train the propensity score model and generate scores.\n",
    "\n",
    "After these steps, the actual Propensity Score Matching would be performed. This typically involves using the propensity scores to match treated units with control units (e.g., via nearest neighbor matching). Once matched pairs are formed, covariate balance between the groups should be checked. Finally, the causal effect (e.g., Average Treatment Effect on the Treated - ATT) is estimated by comparing outcomes (`star_rating`) in the matched treated and control groups. For this draft, the matching and effect estimation part will be a conceptual placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa02e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Ensure all previous functions: load_and_explore_data, preprocess_data, engineer_features, \n",
    "# split_data, train_propensity_model, plot_propensity_score_overlap are defined above)\n",
    "\n",
    "def perform_matching_and_estimate_att(propensity_scores: np.ndarray, \n",
    "                                    treatment: pd.Series, \n",
    "                                    outcome: pd.Series,\n",
    "                                    # covariates_df: pd.DataFrame, # For balance check\n",
    "                                    method='nearest', caliper=0.05):\n",
    "    \"\"\"Conceptual placeholder for matching and ATT estimation.\"\"\"\n",
    "    print(\"\\n--- Conceptual Matching and ATT Estimation ---\")\n",
    "    if propensity_scores is None or treatment is None or outcome is None:\n",
    "        print(\"Insufficient data for matching.\")\n",
    "        return None\n",
    "\n",
    "    match_data = pd.DataFrame({\n",
    "        'propensity_score': propensity_scores,\n",
    "        'treatment': treatment.values, # ensure numpy array for direct indexing\n",
    "        'outcome': outcome.values\n",
    "    })\n",
    "\n",
    "    treated_units = match_data[match_data['treatment'] == 1]\n",
    "    control_units = match_data[match_data['treatment'] == 0]\n",
    "\n",
    "    if treated_units.empty or control_units.empty:\n",
    "        print(\"Not enough treated or control units.\")\n",
    "        return None\n",
    "\n",
    "    # Simplified 1-to-1 nearest neighbor matching with caliper (for illustration)\n",
    "    matched_control_outcomes = []\n",
    "    available_controls = control_units.copy().sort_values(by='propensity_score')\n",
    "    \n",
    "    for _, treated_unit in treated_units.iterrows():\n",
    "        potential_matches = available_controls[\n",
    "            np.abs(available_controls['propensity_score'] - treated_unit['propensity_score']) <= caliper\n",
    "        ]\n",
    "        if not potential_matches.empty:\n",
    "            best_match_idx = (potential_matches['propensity_score'] - treated_unit['propensity_score']).abs().idxmin()\n",
    "            matched_control_outcomes.append(available_controls.loc[best_match_idx, 'outcome'])\n",
    "            available_controls.drop(best_match_idx, inplace=True) # No replacement\n",
    "\n",
    "    if not matched_control_outcomes:\n",
    "        print(\"No matches found.\")\n",
    "        return None\n",
    "\n",
    "    att = treated_units['outcome'].mean() - np.mean(matched_control_outcomes)\n",
    "    print(f\"Estimated ATT: {att:.4f} (based on {len(matched_control_outcomes)} matched pairs)\")\n",
    "    # Further steps: covariate balance check on matched sample.\n",
    "    return att\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution pipeline for the PSM experiment.\"\"\"\n",
    "    file_path = 'review_data.csv'\n",
    "    \n",
    "    # 1. Load & Explore\n",
    "    raw_df = load_and_explore_data(file_path)\n",
    "    if raw_df is None: return\n",
    "\n",
    "    # 2. Preprocess\n",
    "    preprocessed_df = preprocess_data(raw_df)\n",
    "    if preprocessed_df is None or preprocessed_df.empty: \n",
    "        print(\"Preprocessing failed or resulted in empty dataframe.\")\n",
    "        return\n",
    "\n",
    "    # 3. Feature Engineering\n",
    "    featured_df = engineer_features(preprocessed_df)\n",
    "    if featured_df is None or featured_df.empty:\n",
    "        print(\"Feature engineering failed or resulted in empty dataframe.\")\n",
    "        return\n",
    "\n",
    "    # Define columns for splitting and modeling\n",
    "    # Base covariates list (original 'product_category' will be handled by split_data if one-hot encoded)\n",
    "    COVARIATES_BASE = ['price', 'review_age_days', 'review_length', 'sentiment_score_basic', 'product_category']\n",
    "    TREATMENT_COL = 'mention_shipping'\n",
    "    OUTCOME_COL = 'star_rating'\n",
    "    \n",
    "    # Ensure essential columns exist before proceeding\n",
    "    if any(col not in featured_df.columns for col in [TREATMENT_COL, OUTCOME_COL]):\n",
    "        print(f\"Missing treatment or outcome column in featured_df. Needed: '{TREATMENT_COL}', '{OUTCOME_COL}'\")\n",
    "        return\n",
    "\n",
    "    # 4. Data Splitting\n",
    "    try:\n",
    "        X_train, T_train, Y_train, X_val, T_val, Y_val, X_test, T_test, Y_test = split_data(\n",
    "            featured_df, TREATMENT_COL, OUTCOME_COL, COVARIATES_BASE, \n",
    "            test_size=0.2, val_size=0.1 # val_size is % of original for this func\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during data splitting: {e}\")\n",
    "        return\n",
    "        \n",
    "    if X_train.empty:\n",
    "        print(\"Training set is empty after split. Aborting.\")\n",
    "        return\n",
    "\n",
    "    # 5. Propensity Score Model Training\n",
    "    ps_model, ps_train, ps_val, ps_test, _ = train_propensity_model(\n",
    "        X_train, T_train, X_val if not X_val.empty else None, X_test if not X_test.empty else None\n",
    "    )\n",
    "    \n",
    "    if ps_model and ps_train is not None:\n",
    "        print(\"\\nPropensity score model trained.\")\n",
    "        plot_propensity_score_overlap(ps_train, T_train, \"Propensity Score Overlap (Training Set)\")\n",
    "        if ps_val is not None and not T_val.empty:\n",
    "            plot_propensity_score_overlap(ps_val, T_val, \"Propensity Score Overlap (Validation Set)\")\n",
    "        if ps_test is not None and not T_test.empty:\n",
    "            plot_propensity_score_overlap(ps_test, T_test, \"Propensity Score Overlap (Test Set)\")\n",
    "\n",
    "        # 6. Conceptual PSM Application (e.g., on test set)\n",
    "        if ps_test is not None and not T_test.empty and not Y_test.empty:\n",
    "            perform_matching_and_estimate_att(ps_test, T_test, Y_test) # covariates_df=X_test\n",
    "        else:\n",
    "            print(\"Skipping ATT estimation on test set due to missing propensity scores or data.\")\n",
    "    else:\n",
    "        print(\"Propensity score model training failed.\")\n",
    "        \n",
    "    print(\"\\n--- Pipeline Finished ---\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
